{"cells":[{"cell_type":"markdown","metadata":{"id":"uKRxGFV-7p_g"},"source":["<b><font size=\"6\"><u>Neural Networks</u></font></b>\n","\n","In this notebook we are going to check some of the most important parameters that can influence the performance of a neural network.\n","\n","# <font color='#BFD72F'>Contents</font> <a class=\"anchor\" id=\"toc\"></a>\n","\n","* [1 - Initial Steps](#first-bullet)\n","    * [1.1 - Connect to Google Colab](#first-bullet)\n","    * [1.2 - Importing Libraries and Data](#import)\n","    * [1.3 - Data Understanding](#understand)\n","    * [1.4 - Split the Data](#split)\n","    * [1.5 - Preparing Functions](#func)\n","* [2 - Neural Networks](#nn)\n","    * [2.1 - The hidden layer size](#hidden)\n","    * [2.2 - The activation function](#activation)\n","    * [2.3 - The solver](#solver)\n","    * [2.4 - The learning rate initialization](#lr_init)\n","    * [2.5 - The learning rate](#lr)\n","    * [2.6 - The maximum iterations](#max_iter)\n","    * [2.7 - Other parameters](#other)\n","* [3 - RandomizedSearch](#randsearch)\n","* [4 - Extra: Pipeline](#pipe)"]},{"cell_type":"markdown","metadata":{"id":"277TYKvT8Nav"},"source":["# <font color='#BFD72F'>1. Initial Steps</font> <a class=\"anchor\" id=\"first-bullet\"></a>"]},{"cell_type":"markdown","metadata":{"id":"Rnetuq2D7p_l"},"source":["## <font color='#BFD72F'>1.1. Connect to Google Colab</font> <a class=\"anchor\" id=\"first-bullet\"></a>\n","[Back to Contents](#toc)\n","\n","**Step 1 -** Connect the google colab notebook with your google drive. Before running the code below, make sure you have this notebook in the folders mentioned in the variable `path`.<br>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KSHbQIli7p_m"},"outputs":[],"source":["# Connect Google Colab to Drive\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","path = '/content/drive/MyDrive/Colab Notebooks/DM2/LAB05 - Neural Networks/'"]},{"cell_type":"markdown","metadata":{"id":"fNVV_-mn7p_o"},"source":["## <font color='#BFD72F'>1.2. Importing Libraries and Data</font><a class=\"anchor\" id=\"import\"></a>\n","[Back to Contents](#toc)\n","\n","**Step 2 -** Import the needed libraries."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2fyCyORR7p_p"},"outputs":[],"source":["import time\n","from sklearn.neural_network import MLPClassifier\n","import pandas as pd\n","from sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV\n","import numpy as np\n","\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","metadata":{"id":"QonareZ-7p_q"},"source":["**Step 3 -** Import the data that is going to be used into `pandas` dataframes.\n","\n","**Step 3.1 -** Import and check the Insurance dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qlwLUMy07p_q"},"outputs":[],"source":["insurance = pd.read_csv(path + 'data/insurance.csv')\n","\n","#drop useless first columns\n","insurance.drop(columns=\"Unnamed: 0\", inplace =True)\n","\n","#take a look at the data\n","insurance"]},{"cell_type":"markdown","metadata":{"id":"Sj97YnCd7p_r"},"source":["<font color='orange'>____GOAL____  : </font> Predict if the health plan will get an upgrade using the given features by the customer\n","\n","The original dataset contains 2500 rows of hospitalization data that an insurance company is analysing, where the Insurance charges are given against the following attributes of the insured: *Age*, *Sex*, *BMI*, *Number of Children*, *Smoker*, *Region*, etc.\n","The categorical variables have already been encoded and converted into dummy variables for you.\n","\n","\n","`Insured ID`<br>\n","`Year_Birth` -  Insurance contractor year of birth<br>\n","`Gender` - (dummies) Insurance contractor gender, female / male.<br>\n","`Region` - (dummies) The beneficiary's residential area in the US, northeast, southeast, southwest, northwest.<br>\n","`Marital Status` - (dummies) Insurance contractor marital status.<br>\n","`Smoker` - (dummies) Smoker / No smoker.<br>\n","`Income` - Insurance contractor income.<br>\n","`BMI` - Body mass index<br>\n","`BSA` - body surface area.<br>\n","`Insured_Satisfaction` - Insured satisfaction regarding insurance assistance/services covered during hospitalization.<br>\n","`Expenses` - Individual medical costs billed by health insurance.<br>\n","`Expenses in percentage (%)` by categories (Treatment / Medication / Medical_Assistance / Exams / Ambulance_Transport).<br>\n","`Expenses coverage percentage (%)` (Insurance_Coverage / Patient_Coverage).<br>\n","`Plan_Option` Type of plan insurance <br>\n","`Upgrade Health Plan` The customer upgraded the health plan insurance\n","<- <font color='orange'> **Dependent Variable / Target** </font>"]},{"cell_type":"markdown","metadata":{"id":"LDjNYf808Na6"},"source":["## <font color='#BFD72F'> 1.3. Data Understanding</font> <a class=\"anchor\" id=\"understand\"></a>\n","[Back to Contents](#toc)"]},{"cell_type":"markdown","metadata":{"id":"-B9PGi4g8Na8"},"source":["**Step 4 -** Use the method `info()` to look at datatypes and missing values."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vBuLOcd58Na8"},"outputs":[],"source":["insurance.info()"]},{"cell_type":"markdown","metadata":{"id":"Ri4_qZkA8Na9"},"source":["**Step 5 -** Use the method `describe()` to look at the descriptive statistics of each one of your variables."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dlEWw4Mg8Na9"},"outputs":[],"source":["insurance.describe().T"]},{"cell_type":"markdown","metadata":{"id":"8cI3bKSv8Na9"},"source":["**Step 6 -** Use the method value_counts() to look at the class distribution of the dependent variable."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vuGPHb-Q8Na9"},"outputs":[],"source":["insurance['Upgrade_Health_Plan'].value_counts()"]},{"cell_type":"markdown","metadata":{"id":"ChEr6Tmv8Na9"},"source":["## <font color='#BFD72F'> 1.4. Split the Data</font> <a class=\"anchor\" id=\"split\"></a>\n","[Back to Contents](#toc)"]},{"cell_type":"markdown","metadata":{"id":"wO0CKyFa7p_s"},"source":["**Step 7** - Define the independent variables as __X__ and the dependent variable as __y__."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l4qhx91i7p_s"},"outputs":[],"source":["X = insurance.loc[:, insurance.columns != 'Upgrade_Health_Plan']\n","y = insurance['Upgrade_Health_Plan']"]},{"cell_type":"markdown","metadata":{"id":"lnzP_DJ47p_u"},"source":["**Step 8** - By using the method `train_test_split`, split the data into X_train, X_test, y_train and y_test, defining `test_size` as 30%, `random_state` equal to 150 and `stratify` by the target."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jpbSq6ps7p_u"},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(X,\n","                                                    y,\n","                                                    test_size = 0.3,\n","                                                    random_state = 150,\n","                                                    shuffle = True,\n","                                                    stratify = y)"]},{"cell_type":"markdown","metadata":{"id":"iWo7AfUY7p_v"},"source":["## <font color='#BFD72F'> 1.5. Preparing Functions</font> <a class=\"anchor\" id=\"func\"></a>\n","[Back to Contents](#toc)"]},{"cell_type":"markdown","metadata":{"id":"t2TmELGP7p_v"},"source":["**Step 9** - Create a function named __avg_score__ that will return the average score value for the train and the test set, applying a certain model (is this notebook we will be using neural networks) and using K-fold cross-validation. It should also measure the model's fitting time and the number of iterations needed by the model.<br>\n","The function will receive as parameters the model and the number of folds to be used."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3mgPHZZy7p_v"},"outputs":[],"source":["def avg_score(model, number_splits):\n","\n","    # create lists to store the results from the different neural networks\n","    score_train = []\n","    score_test = []\n","    timer = []\n","    n_iter = []\n","\n","    # apply kfold with the pre-defined number_splits\n","    kf = KFold(n_splits=number_splits)\n","\n","    for train_index, test_index in kf.split(X):\n","\n","        # get the indexes of the observations assigned for each partition\n","        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n","        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n","\n","        #---> start counting time\n","        begin = time.perf_counter()\n","\n","        # fit the model to the data\n","        model.fit(X_train, y_train)\n","\n","        #---> finish counting time\n","        end = time.perf_counter()\n","\n","        # check the mean accuracy for the train\n","        value_train = model.score(X_train, y_train)\n","        # check the mean accuracy for the test\n","        value_test = model.score(X_test,y_test)\n","\n","        # append the accuracies, the time and the number of iterations in the corresponding list\n","        score_train.append(value_train)\n","        score_test.append(value_test)\n","        timer.append(end-begin)\n","        n_iter.append(model.n_iter_)\n","\n","    # calculate the average and the std for each measure (accuracy, time and number of iterations)\n","    ### AVG\n","    avg_time = round(np.mean(timer),3)\n","    avg_train = round(np.mean(score_train),3)\n","    avg_test = round(np.mean(score_test),3)\n","    avg_iter = round(np.mean(n_iter),1)\n","    ### STD\n","    std_time = round(np.std(timer),2)\n","    std_train = round(np.std(score_train),2)\n","    std_test = round(np.std(score_test),2)\n","    std_iter = round(np.std(n_iter),1)\n","\n","    return str(avg_time) + '+/-' + str(std_time), str(avg_train) + '+/-' + str(std_train),\\\n","str(avg_test) + '+/-' + str(std_test), str(avg_iter) + '+/-' + str(std_iter)"]},{"cell_type":"markdown","metadata":{"id":"JJ7WxieW7p_v"},"source":["**Step 10** - Create a function named __show_results__ that will return the average score for the train and test dataset (returned from the function __avg_score__) for several given models."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sCkmBXYD7p_w"},"outputs":[],"source":["def show_results(df, *args, number_splits):\n","    \"\"\"\n","    Receive an empty dataframe and the different models and call the function avg_score\n","    \"\"\"\n","    count = 0\n","    # for each model passed as argument\n","    for arg in args:\n","        # obtain the results provided by avg_score\n","        time, avg_train, avg_test, avg_iter = avg_score(arg, number_splits)\n","        # store the results in the right row\n","        df.iloc[count] = time, avg_train, avg_test, avg_iter\n","        count+=1\n","    return df"]},{"cell_type":"markdown","metadata":{"id":"c3DbXe5I7p_w"},"source":["# <font color='#BFD72F'>2. Neural Networks</font> <a class=\"anchor\" id=\"nn\"></a>\n","[Back to Contents](#toc)\n","\n","Neural network is a type of machine learning model that's designed to mimic the way the human brain works.<br>\n","It can be used for a wide range of tasks, such as recognizing images, understanding natural language, playing games, making medical diagnoses, etc.\n","\n","\n","<font size=6>The Structure of Neural Network</font>\n","\n","Neural Networks are composed of layers of nodes, or \"neurons,\" each of which performs a simple computation and passes the results to the next layer.<br>\n","\n","__Input Layer__: This is the initial data for the network. Each node in this layer represents a feature of the input data.<br>\n","\n","__Hidden Layer__: These are the layers between the input and output layer. They are referred to as \"hidden\" because we don't see their input-output relationships directly from the data; they are internally computed. The nodes in these layers apply weights to the inputs and pass them through an `activation function`, which decides whether a neuron should be activated based on the weighted sum.<br>\n","\n","__Output Layer__ The output layer produces the final predictions or classifications made by the network. The structure of the output layer depends on the type of problem. For instance, for a __binary classification__ problem, the output layer would have one neuron with a `sigmoid activation function` to predict two classes.\n","For a __multi-class problem__, the output layer would have as many neurons as the number of classes with a `softmax activation function` to predict probabilities of different classes.\n","\n","\n","A neural network learns by adjusting the weights applied in the hidden layers. During the training process, the network is given a set of inputs and the desired outputs. It makes a prediction based on the current weights, and then adjusts the weights to minimize the difference between the predicted and actual outputs. This process is usually accomplished through a method called __backpropagation__ and an __optimization strategy__ like stochastic gradient descent."]},{"cell_type":"markdown","metadata":{"id":"T7DJWRc67p_w"},"source":["<img src=\"https://drive.google.com/uc?id=1e14l1uAqWEedSwDVVruCBrsgrtMJdNQC\" width=\"400px\"> <img name=\"neuralnetwork.gif\">"]},{"cell_type":"markdown","metadata":{"id":"WM4LRA027p_w"},"source":["**Step 11** - Create an instance of MLPClassifier with the default parameters and name it as __model__. Check the results using the above created functions."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jqs2KUki7p_x"},"outputs":[],"source":["model = MLPClassifier()\n","df = pd.DataFrame(columns = ['Time','Train','Test', 'Iterations'], index = ['Raw'])\n","show_results(df, model, number_splits=10)"]},{"cell_type":"markdown","metadata":{"id":"V8e3-0Mn7p_x"},"source":["## <font color='#BFD72F' id=\"hidden\"> 2.1. The hidden layer size </font>\n","[Back to Contents](#toc)\n","\n","__Determine the Number of hidden layers and neurons__<br>\n","By default, one hidden layer with 100 neurons, which is represented as:<br>\n","__(100,)__"]},{"cell_type":"markdown","metadata":{"id":"t6MSKmzm7p_x"},"source":["__The number of hidden layers__: <br>\n","-\tIncreasing the number of hidden layers might or might not improve the accuracy, it depends on the complexity of the problem\n","-\tIncreasing the number of hidden layers more than needed will cause overfit on the training set and a decrease in the accuracy value for the test set\n","\n","__The number of hidden units__: <br>\n","-\tUsing too few neurons in the hidden layers will result in underfitting\n","-\tUsing too many neurons in the hidden layer may result in overfitting and increases the training time of the neural network\n","\n","The aim is to keep a good trade-off between the simplicity of the model and the performance accuracy! <br>\n","<div class=\"alert alert-success\">Different rules of thumb exist (take them with a grain of salt): <br>\n","\n","-\tThe number of hidden neurons should be __between the size of the input layer and the size of the output layer__\n","-\tThe number of hidden neurons should be __2/3 the size of the input layer, plus the size of the output layer__\n","-\tThe number of hidden neurons should be __less than twice the size of the input layer__</div>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Vbr1KQVX7p_y"},"source":["**Step 12** - Create an `MLPClassifier` with one hidden layer and one neuron and name it __model_simple__."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tzc6lUI77p_y"},"outputs":[],"source":["model_simple = MLPClassifier(hidden_layer_sizes=(1))"]},{"cell_type":"markdown","metadata":{"id":"N_RUtRV87p_y"},"source":["**Step 13** - Create an `MLPClassifier` with one hidden layer and 10 neurons and name it __model_medium__."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G2b-EGTs7p_z"},"outputs":[],"source":["model_medium = MLPClassifier(hidden_layer_sizes=(10))"]},{"cell_type":"markdown","metadata":{"id":"y5MIDUBe7p_0"},"source":["**Step 14** - Create an `MLPClassifier` with four hidden layers and 100 neurons each and name it __model_complex__."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O193f8tX7p_0"},"outputs":[],"source":["model_complex = MLPClassifier(hidden_layer_sizes = (100,100,100,100))"]},{"cell_type":"markdown","metadata":{"id":"0Z2XIjrq7p_1"},"source":["**Step 15** - Check the mean accuracy of each model by calling the function _show_results_ and pass as arguments the dataset and the three models."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bF6P_6Gz7p_2"},"outputs":[],"source":["df = pd.DataFrame(columns = ['Time','Train','Test', 'Iterations'], index = ['Simple','Medium','Complex'])\n","show_results(df, model_simple, model_medium, model_complex, number_splits= 10)"]},{"cell_type":"markdown","metadata":{"id":"D-C6WZYz7p_2"},"source":["While the results may differ in different runs, at the end will arive at the following conclusions:\n","- The more complex the model, the higher the running time;\n","- We can boost the performance of our model by correctly adjusting the complexity of it - too simple and it will underfit, too complex and it might overfit."]},{"cell_type":"markdown","metadata":{"id":"Bi9-qNuo7p_2"},"source":["## <font color='#BFD72F' id=\"activation\"> 2.2. The activation function</font>\n","[Back to Contents](#toc)\n","\n","__(default = 'relu')__\n","\n","Check this <a href=\"https://www.analyticsvidhya.com/blog/2021/04/activation-functions-and-their-derivatives-a-quick-complete-guide/\">link</a> for more information regarding the advantages and disadvantages of different activation functions."]},{"cell_type":"markdown","metadata":{"id":"IwgzjXNe7p_3"},"source":["<img src=\"https://drive.google.com/uc?id=1IqkVu_b-NLJtBfguWwbuzgn46GG9IlgF\" width=\"400px\"> <img name=\"activation.png\">"]},{"cell_type":"markdown","metadata":{"id":"chr77uHn7p_3"},"source":["**Step 16** - Create an instance of `MLPClassifier`, define the activation as _relu_ and name it as __model_relu__."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EflS25le7p_4"},"outputs":[],"source":["model_relu = MLPClassifier(activation = 'relu')"]},{"cell_type":"markdown","metadata":{"id":"bm4E_bxE7p_4"},"source":[" - __Advantages:__\n","     - Computationally efficient - allows the network to converge very quickly.\n"," - __Disadvantages:__\n","     - The dying ReLU problem - When inputs approach zero, or are negative, the gradient of the function becomes zero and the network cannot perform backpropagation and cannot learn."]},{"cell_type":"markdown","metadata":{"id":"U5boVyxd7p_5"},"source":["**Step 16.1** - Use the `.activation` attribute to check the activation function used for hidden layer."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k6rg2Neo7p_5","scrolled":false},"outputs":[],"source":["model_relu.activation"]},{"cell_type":"markdown","metadata":{"id":"9ZwjEqVu7p_6"},"source":["**Step 17** - Create an instance of `MLPClassifier`, define the activation as _logistic_ (sigmoid) and name it as __model_logistic__."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Iw_jhXtx7p_6"},"outputs":[],"source":["model_logistic = MLPClassifier(activation = 'logistic')"]},{"cell_type":"markdown","metadata":{"id":"FdAF77bX7p_6"},"source":[" - __Advantages:__\n","     - Smooth gradient, preventing “jumps” in output values.\n","     - Output values bound between 0 and 1, normalizing the output of each neuron.\n"," - __Disadvantages:__\n","     - Vanishing gradient—for very high or very low values of X, there is almost no change to the prediction, causing a vanishing gradient problem. This can result in the network refusing to learn further, or have slow convergence.\n","     - Computationally expensive."]},{"cell_type":"markdown","metadata":{"id":"oa37_mAl7p_7"},"source":["**Step 18** - Create an instance of `MLPClassifier`, define the activation as _tanh_ and name it as __model_tanh__."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pUEQmS4T7p_7"},"outputs":[],"source":["model_tanh = MLPClassifier(activation = 'tanh')"]},{"cell_type":"markdown","metadata":{"id":"iBhbG9yf7p_7"},"source":[" - __Advantages:__\n","     - Zero centered - making it easier to model inputs that have strongly negative, neutral and strongly positive values. Other than that it is similar to the sigmoid function. <br>\n"," - __Disadvantages:__\n","     - Same as with the sigmoid function."]},{"cell_type":"markdown","metadata":{"id":"f_L5z5nT7p_7"},"source":["**Step 19** - Check the mean accuracy of each model by calling the function _show_results_ and pass as arguments the dataset and the three models."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j0LVTiAh7p_8"},"outputs":[],"source":["df = pd.DataFrame(columns = ['Time','Train','Test', 'Iterations'], index = ['relu','logistic','tanh'])\n","show_results(df, model_relu, model_logistic, model_tanh, number_splits= 10)"]},{"cell_type":"markdown","metadata":{"id":"giJD0mjB7p_8"},"source":["Checking the results, we can identify some evidences:\n","- Relu tends to be faster than logistic or tanh.\n","- Sigmoid functions and their variations (such as tanh) generally work better in the case of classification problems."]},{"cell_type":"markdown","metadata":{"id":"jY3D2eamIrfg"},"source":["**Step 19.1** - Use the `out_activation_` attribute to check the activation function used for the output layer. <br>\n","Note: you can see this attribute after fitting the model to the data ex:  model.fit(X_train, y_train)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r0PTUErSIw_A"},"outputs":[],"source":["model_relu.out_activation_"]},{"cell_type":"markdown","metadata":{"id":"1vecwRp67p_8"},"source":["## <font color='#BFD72F' id=\"solver\"> 2.3. The solver </font>\n","[Back to Contents](#toc)\n","\n","__default = 'adam'__\n","\n","For more information check this <a href=\"http://www.robotics.stanford.edu/~ang/papers/icml11-OptimizationForDeepLearning.pdf\">paper</a>."]},{"cell_type":"markdown","metadata":{"id":"wIMch4yO7p_8"},"source":["**Step 20** - Create an instance of `MLPClassifier`, define the solver as _sgd_ and name ir as __model_sgd__."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4Oe_2Y_O7p_8"},"outputs":[],"source":["model_sgd = MLPClassifier(solver = 'sgd')"]},{"cell_type":"markdown","metadata":{"id":"5zlHxxk_7p_9"},"source":["Notes:\n","- While Gradient Descent use the whole training data to do a single update, in SGD a random data point of the training data to update the parameters - SGD is faster than GD.\n","- It uses a common learning rate for all parameters, contrarialy to what happens in Adam."]},{"cell_type":"markdown","metadata":{"id":"hw-jK8967p_9"},"source":["**Step 21** - Create an instance of `MLPClassifier`, define the solver as _adam_ and name it as __model_adam__.\n","<br>\n","\n","When to use: It achieves good results fast, therefore is a good option for complex models, if processing time is an issue."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-Xah6NaQ7p_9"},"outputs":[],"source":["model_adam = MLPClassifier(solver = 'adam')"]},{"cell_type":"markdown","metadata":{"id":"kxj2fyAy7p_9"},"source":["Notes:\n","- It computes individual adaptive learning rates for different parameters.\n","- Adam combines the advantages of RMSProp and AdaGrad."]},{"cell_type":"markdown","metadata":{"id":"872BfluV7p_9"},"source":["**Step 22** - Check the mean accuracy of each model by calling the function _show_results_ and pass as arguments the dataset and the three models."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uo4nh-p-7p_-"},"outputs":[],"source":["df = pd.DataFrame(columns = ['Time','Train','Test', 'Iterations'], index = ['sgd','adam'])\n","show_results(df, model_sgd, model_adam,number_splits= 10)"]},{"cell_type":"markdown","metadata":{"id":"psR6Ikhc7p_-"},"source":["In sklearn, the number of iterations for __sgd__ and __adam__ correspond to the number of epochs (an epoch consists of one full cycle through the training data)."]},{"cell_type":"markdown","metadata":{"id":"-DqXFC_Y8NbL"},"source":["<img src=\"https://drive.google.com/uc?id=1sq7a1PigaAptGEF0lA-UL3cKoGnesYb0\" width=\"500px\"> <img name=\"optimizers.gif\">\n","\n","For more information check this <a href=\"https://www.analyticsvidhya.com/blog/2021/10/a-comprehensive-guide-on-deep-learning-optimizers/\">webpage</a>.\n"]},{"cell_type":"markdown","metadata":{"id":"T4uOXQ1P7p_-"},"source":["## <font color='#BFD72F' id=\"lr_init\"> 2.4. The learning rate initialization </font>\n","[Back to Contents](#toc)\n","\n","__Only for sgd and adam (default = 0.001)__\n","\n","The learning rate is one of the most important hyper-parameters to tune for training deep neural networks:\n","\n","__Small LR__:\n","- If the learning rate is small, then training is more reliable, but optimization will take a lot of time because steps towards the minimum of the loss function are tiny - a smaller learning rate may allow the model to learn a more optimal or even globally optimal set of weights but may take significantly longer to train.\n","- A learning rate that is too small may never converge or may get stuck on a suboptimal solution.\n","\n","__Big LR__:\n","- If the learning rate is high, then training may not converge or even diverge. Weight changes can be so big that the optimizer overshoots the minimum and makes the loss worse - a large learning rate allows the model to learn faster, at the cost of arriving on a sub-optimal final set of weights.\n","\n","The training should start with a relatively large learning rate because, in the beginning, random weights are far from optimal, and then the learning rate should decrease during training to allow for more fine-grained weight updates.\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1Y5wLeNa8pvtCst54Ch5oSpkewSQzWc90\" width=\"800px\"> <img name=\"lr.png\">"]},{"cell_type":"markdown","metadata":{"id":"qZm0ccwd7p__"},"source":["**Step 23** - Create an instance of `MLPClassifier`, define the solver as _sgd_, the learning_rate_init as _0.5_ and name it as __model_lr_big__."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Fpsq9wh7p__"},"outputs":[],"source":["model_lr_big = MLPClassifier(solver = 'sgd', learning_rate_init = 0.5)"]},{"cell_type":"markdown","metadata":{"id":"17BDvEDr7p__"},"source":["**Step 24** - Create an instance of `MLPClassifier`, define the solver as _sgd_, the learning_rate_init as _0.001_ and name it as __model_lr_medium__."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2uyi6gEg7p__"},"outputs":[],"source":["model_lr_medium = MLPClassifier(solver = 'sgd', learning_rate_init = 0.001)"]},{"cell_type":"markdown","metadata":{"id":"P4stV57o7qAA"},"source":["**Step 25** - Create an instance of `MLPClassifier`, define the solver as _sgd_, the learning_rate_init as _0.000001_ and name it as __model_lr_small__."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4U8H8Acc7qAA"},"outputs":[],"source":["model_lr_small = MLPClassifier(solver = 'sgd', learning_rate_init = 0.000001)"]},{"cell_type":"markdown","metadata":{"id":"EipMeQoV7qAA"},"source":["**Step 26** - Check the mean accuracy of each model by calling the function _show_results_ and pass as arguments the dataset and the three models."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rWtQkaN37qAA"},"outputs":[],"source":["df = pd.DataFrame(columns = ['Time','Train','Test', 'Iterations'], index = ['big','medium','small'])\n","show_results(df, model_lr_big, model_lr_medium, model_lr_small,number_splits= 10)"]},{"cell_type":"markdown","metadata":{"id":"RTxbZTvc7qAB"},"source":["## <font color='#BFD72F' id=\"lr\"> 2.5. The learning rate</font>\n","[Back to Contents](#toc)\n","\n","__Only for sgd (default = 'constant')__"]},{"cell_type":"markdown","metadata":{"id":"69-RDSs77qAB"},"source":["**Step 27** - Create an instance of `MLPClassifier`, define the solver as _sgd_, the learning_rate as _constant_ and name it as __model_constant__.\n","\n","Definition: If the learning rate is constant, as the name says, the learning rate will always remain equal to the initial learning rate."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B_oltZJi7qAB"},"outputs":[],"source":["model_constant = MLPClassifier(solver = 'sgd', learning_rate = 'constant')"]},{"cell_type":"markdown","metadata":{"id":"O11zFWdu7qAB"},"source":["**Step 28** - Create an instance of `MLPClassifier`, define the solver as _sgd_, the learning_rate as _invscaling_ and name it as __model_invscaling__.\n","\n","Definition: If the learning rate is invscaling, it gradually decreases the learning rate at each time step ‘t’ using an inverse scaling exponent of ‘power_t’.\n","\n","$$\n","effective\\,learning\\,rate = \\frac{learning\\_rate\\_init}{t^{\\,power\\_t}}\n","$$\n","\n","Note: The __power_t__ (default = 0.5) is another parameter that you can change."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KKkFviBC7qAB"},"outputs":[],"source":["model_invscaling = MLPClassifier(solver = 'sgd', learning_rate = 'invscaling')"]},{"cell_type":"markdown","metadata":{"id":"-g4bcVSl7qAC"},"source":["**Step 29** - Create an instance of `MLPClassifier`, define the solver as _sgd_, the learning_rate as _adaptive_ and name it as __model_adaptive__.\n","\n","Definition: <br>\n","If the learning rate is adaptive, then it keeps the learning rate constant to ‘learning_rate_init’ as long as training loss keeps decreasing. <br>\n","Each time two consecutive epochs fail to decrease training loss by at least __tol__ (tolerance for the optimization - another parameter that you can change), or fail to increase validation score by at least __tol__ if __early_stopping__ (meaning to terminate training when the validation score is not improving - another parameter that you can change) is on, the current learning rate is divided by 5."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eekrfCit7qAC"},"outputs":[],"source":["model_adaptive = MLPClassifier(solver = 'sgd', learning_rate = 'adaptive')"]},{"cell_type":"markdown","metadata":{"id":"Ud6NHgvy7qAC"},"source":["**Step 30** - Check the mean accuracy of each model by calling the function _show_results_ and pass as arguments the dataset and the three models."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"47Or9AsV7qAC"},"outputs":[],"source":["df = pd.DataFrame(columns = ['Time','Train','Test', 'Iterations'], index = ['constant','invscaling','adaptive'])\n","show_results(df, model_constant, model_invscaling, model_adaptive,number_splits= 10)"]},{"cell_type":"markdown","metadata":{"id":"drcoYMXX7qAF"},"source":["## <font color='#BFD72F' id=\"max_iter\"> 2.6. The maximum iterations </font>\n","[Back to Contents](#toc)"]},{"cell_type":"markdown","metadata":{"id":"OHQBwDyg7qAG"},"source":["By default, sklearn defines the maximum number of iterations as 200. While this could be enough for simple datasets, in complex problems you should try values higher that allow the model to converge."]},{"cell_type":"markdown","metadata":{"id":"SWJsB2jf7qAG"},"source":["**Step 31** - Create an instance of `MLPClassifier`, define the max_iter as _20_ and name it as __model_maxiter_20__."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nPeMxndk7qAG"},"outputs":[],"source":["model_maxiter_20 = MLPClassifier(max_iter = 20)"]},{"cell_type":"markdown","metadata":{"id":"grScf1ZI7qAH"},"source":["**Step 32** - Create an instance of `MLPClassifier`, define the max_iter as _100_ and name it as __model_maxiter_100__."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nltntmTO7qAH"},"outputs":[],"source":["model_maxiter_100 = MLPClassifier(max_iter = 100)"]},{"cell_type":"markdown","metadata":{"id":"0KgcU0Rn7qAH"},"source":["**Step 33** - Create an instance of `MLPClassifier`, define the max_iter as _500_ and name it as __model_maxiter_500__."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6vSdfi667qAI"},"outputs":[],"source":["model_maxiter_500 = MLPClassifier(max_iter = 500)"]},{"cell_type":"markdown","metadata":{"id":"XaO2FFiu7qAI"},"source":["**Step 34** - Check the mean accuracy of each model by calling the function _show_results_ and pass as arguments the dataset and the three models."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yZdY8bvm7qAI","scrolled":true},"outputs":[],"source":["df = pd.DataFrame(columns = ['Time','Train','Test', 'Iterations'], index = ['max iter 20','max iter 100','max iter 200'])\n","show_results(df, model_maxiter_20, model_maxiter_100, model_maxiter_500,number_splits= 10)"]},{"cell_type":"markdown","metadata":{"id":"x_WcKM6i7qAI"},"source":["## <font color='#BFD72F' id=\"other\"> 2.7. Other parameters</font>"]},{"cell_type":"markdown","metadata":{"id":"5vdAii087qAI"},"source":["|Parameter| Definition | LBFGS | SGD | ADAM |\n","|---|---|---|---|---|\n","|alpha| L2 penalty (regularization term) parameter | yes | yes | yes |\n","| power_t | The exponent for inverse scaling learning rate. It is used in updating effective learning rate when the learning_rate is set to ‘invscaling’. | no | yes | no |\n","| shuffle | Whether to shuffle samples in each iteration. | no | yes | yes |\n","| tol | Tolerance for the optimization. When the loss or score is not improving by at least tol for n_iter_no_change consecutive iterations, unless learning_rate is set to ‘adaptive’, convergence is considered to be reached and training stops. | yes | yes | yes |\n","| warm_start | When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. | yes | yes | yes |\n","| momentum | Momentum for gradient descent update. Should be between 0 and 1. | no | yes | no |\n","| nesterovs_momentum | Whether to use Nesterov’s momentum.| no | yes | no |\n","| early stopping | Whether to use early stopping to terminate training when validation score is not improving. If set to true, it will automatically set aside 10% of training data as validation and terminate training when validation score is not improving by at least tol for n_iter_no_change consecutive epochs. The split is stratified, except in a multilabel setting.  | no | yes | yes |\n","| validation_fraction | The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if early_stopping is True | no | yes | yes|\n","| beta1 | Exponential decay rate for estimates of first moment vector in adam, should be in [0, 1). | no | no | yes |\n","| beta2 | Exponential decay rate for estimates of second moment vector in adam, should be in [0, 1).  | no | no | yes |\n","| epsilon | Value for numerical stability in adam. | no | no | yes |\n","| n_iter_no_change | Maximum number of epochs to not meet tol improvement. |  no | yes | yes |\n","| max_fun | Only used when solver=’lbfgs’. Maximum number of loss function calls. The solver iterates until convergence (determined by ‘tol’), number of iterations reaches max_iter, or this number of loss function calls. | yes | no | no |"]},{"cell_type":"markdown","metadata":{"id":"l12XUfiw7qAJ"},"source":["# <font color='#BFD72F'>3. RandomizedSearch</font> <a class=\"anchor\" id=\"randsearch\"></a>\n","[Back to Contents](#toc)"]},{"cell_type":"markdown","metadata":{"id":"JlRvvfGP7qAJ"},"source":["**Step 35** - Define a dictionary named as __parameter_space__ and define the following options to be considered during modelling:\n","- 'hidden_layer_sizes': [(50,50,50), (100,)\n","- 'activation': ['tanh', 'relu']\n","- 'solver': ['sgd', 'adam']\n","- 'learning_rate_init' : [0.0001, 0.001, 0.01, 0.1]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qEu30dif7qAJ"},"outputs":[],"source":["parameter_space = {\n","    'hidden_layer_sizes': [(50,50,50), (100,)],\n","    'activation': ['tanh', 'relu'],\n","    'solver': ['sgd', 'adam'],\n","    'learning_rate_init': [0.0001, 0.001, 0.01, 0.1]\n","}"]},{"cell_type":"markdown","metadata":{"id":"NWK94duR7qAK"},"source":["**Step 36** - Create an instance of `RandomizedSearchCV` named as __clf__ and pass as parameters the __model__ and the __parameter_space__."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nD_L7gKR7qAK"},"outputs":[],"source":["clf = RandomizedSearchCV(model, parameter_space)"]},{"cell_type":"markdown","metadata":{"id":"x6I7FjmJ7qAK"},"source":["**Step 37** - Fit your instance to X_train and y_train."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ec1pn47_7qAK"},"outputs":[],"source":["clf.fit(X_train, y_train)"]},{"cell_type":"markdown","metadata":{"id":"P2MXnrq07qAL"},"source":["**Step 38** - Call the attribute _.best_params__ to check which is the best combination of parameters."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ehK51ROh8Nbb"},"outputs":[],"source":["clf.best_params_"]},{"cell_type":"markdown","metadata":{"id":"LcxsPZ_C8Nbd"},"source":["**Step 39** - Create a __final_model__ with the best parameters, as checked in previous step, calling the attribute _.best_estimator__."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pgZ8j4U87qAL"},"outputs":[],"source":["final_model = clf.best_estimator_.fit(X_train, y_train)\n","print('Train:', final_model.score(X_train, y_train))\n","print('Test:', final_model.score(X_test, y_test))"]},{"cell_type":"markdown","metadata":{"id":"7VLcMYGA7qAM"},"source":["**Step 40** - Create a loop to check the mean and the standard deviation of the different models created using the different combinations using `RandomizedSearchCV`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q6Ncoij67qAM"},"outputs":[],"source":["# Best parameter set\n","print('------------------------------------------------------------------------------------------------------------------------')\n","print('Best parameters found:\\n', clf.best_params_)\n","print('------------------------------------------------------------------------------------------------------------------------')\n","\n","# All results\n","means = clf.cv_results_['mean_test_score']\n","stds = clf.cv_results_['std_test_score']\n","params = clf.cv_results_['params']\n","\n","for mean, std, param in zip(means, stds, params):\n","    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std , param))"]},{"cell_type":"markdown","metadata":{"id":"9NlcKh_W8Nbe"},"source":["# <font color='#BFD72F'>4. Extra: Pipeline</font> <a class=\"anchor\" id=\"pipe\"></a>\n","[Back to Contents](#toc)\n","\n","\n","\n","A pipeline is a sequence of interconnected __transformers__ and an __estimator__ that encompasses the entire workflow, from data preparation and preprocessing to model training, evaluation, and deployment, as represented as follows:\n","\n","<img src=\"https://drive.google.com/uc?id=1aHVWQpnjJNfdIaPKfm263VyxxB1UzRhC\" width=\"800px\"> <img name=\"pipeline.png\">\n","\n","As defined in the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\">documentation</a>, the __transformers__ must implement `.fit()` and `.transform()` methods while the __estimator__ only needs to implement `.fit()`.\n","\n","The purpose of the pipeline is to assemble several steps that can be cross-validated together while setting different parameters.\n"]},{"cell_type":"markdown","metadata":{"id":"OGioK3Cr8Nbe"},"source":["**Step 41** - Create a pipeline to perform scaling (with `MinMaxScaler`), feature selection (with `SelectKBest`) and hyperparameter tuning (with `GridSearchCV`) for a `RandomForestClassifier`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hm2it3b18Nbe"},"outputs":[],"source":["from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.feature_selection import SelectKBest, f_classif\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import GridSearchCV\n","\n","\n","pipe = Pipeline([\n","    ('scaler', MinMaxScaler()),\n","    ('feature_selection', SelectKBest(f_classif)),\n","    ('classifier', RandomForestClassifier(random_state=5))\n","    ])\n","\n","params = dict(\n","    feature_selection__k=[2, 3, 4],\n","    classifier__max_depth=[10, 20, 30]\n","    )\n","\n","grid_search = GridSearchCV(pipe, param_grid=params, scoring='f1', cv=6)\n","gs = grid_search.fit(X_train, y_train)\n","print(gs.best_params_)\n","print(gs.best_score_)"]},{"cell_type":"markdown","metadata":{"id":"K3qCTsbg8Nbf"},"source":["**Step 41.1** - Can call the `.best_estimator_` attribute to check the full features of the model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lVf09VsP8Nbf"},"outputs":[],"source":["print(gs.best_estimator_)"]},{"cell_type":"markdown","metadata":{"id":"sqMM5_si8Nbg"},"source":["**Step 42** - Use pipeline result to classify `X_test` and measure the F1 score."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"khKj0RGT8Nbg"},"outputs":[],"source":["from sklearn.metrics import f1_score\n","\n","Y_pred = gs.predict(X_test)\n","f1_score(y_test, Y_pred)"]},{"cell_type":"markdown","metadata":{"id":"_j3exdDK8Nbg"},"source":["In the example above there were no preprocessing because the dataset had it already, but you can use pipeline for the full CRISP-DM process.\n","Lets now see an example how to test different scalers.\n","\n","**Step 43** - Adapt the previous pipeline to test wich is the most suitable scaler, `MinMaxScaler` or `StandardScaler`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"THMiZYUT8Nbg"},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler\n","\n","scalers = {\"minmax\": MinMaxScaler(),\n","           \"standard\": StandardScaler()\n","          }\n","\n","args = []\n","results = []\n","clf =[]\n","\n","for scl in scalers.values():\n","    pipe = Pipeline([\n","    ('scaler', scl),\n","    ('feature_selection', SelectKBest(f_classif)),\n","    ('classifier', RandomForestClassifier(random_state=5))\n","    ])\n","\n","    params = dict(\n","        feature_selection__k=[2, 3, 4],\n","        classifier__max_depth=[10, 20, 30]\n","        )\n","\n","    grid_search = GridSearchCV(pipe, param_grid=params, scoring='f1', cv=6)\n","    gs = grid_search.fit(X_train, y_train)\n","    args.append(gs.best_params_)\n","    results.append(gs.best_score_)\n","    clf.append(gs.best_estimator_)\n","\n","print(args)\n","print(results)"]},{"cell_type":"markdown","metadata":{"id":"UGn-C3rx8Nbh"},"source":["**Step 42** - Use the best classifier to classify `X_test` and measure the F1 score."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K21M1mmG8Nbi"},"outputs":[],"source":["y_pred = clf[1].predict(X_test)\n","f1_score(y_test, Y_pred)"]},{"cell_type":"markdown","metadata":{"id":"r4ej0Isi8Nbi"},"source":["<b><font size=\"6\"> Don't forget to practice at home  &#128521;\n","\n","... Questions about the project? </font></b>\n","You may always look for more information on the internet, here's an [example](https://michael-fuchs-python.netlify.app/2020/08/21/the-data-science-process-crisp-dm/)."]}],"metadata":{"colab":{"provenance":[{"file_id":"1uWAbivDT47KQIxW9I1iuZgg9nBwW2qqg","timestamp":1684603081545},{"file_id":"1RnQ3o1DRD0LhaXyl050C5BDFmWfSygS3","timestamp":1684601446754}],"toc_visible":true},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false}},"nbformat":4,"nbformat_minor":0}